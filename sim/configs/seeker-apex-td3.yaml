seeker-apex-td3:
    env: SeekerSimEnv-v0
    run: APEX_DDPG
    checkpoint_freq: 50
    stop:
        episode_reward_mean: 5
    config:
        # === Tricks ====
        twin_q: True
        policy_delay: 2
        smooth_target_policy: True
        act_noise: 0.1
        target_noise: 0.2
        noise_clip: 0.5
        horizon: 100

        # === Model ===
        actor_hiddens: [128, 64]
        critic_hiddens: [128, 64]
        n_step: 4
        model: {}
        gamma: 0.99
        env_config: {}

        # === Exploration ===
        schedule_max_timesteps: 50000000
        exploration_fraction: 0.4
        exploration_final_eps: 0.02
        noise_scale: 0.1
        exploration_theta: 0.15
        exploration_sigma: 0.2
        target_network_update_freq: 50000
        tau: 0.001 # 1

        # === Replay buffer ===
        buffer_size: 2000000
        prioritized_replay: True
        prioritized_replay_alpha: 0.6
        prioritized_replay_beta: 0.4
        prioritized_replay_eps: 0.000001
        clip_rewards: False
        optimizer:
            num_replay_buffer_shards: 8

        # === Optimization ===
        lr: 0.001
        actor_loss_coeff: 0.1
        critic_loss_coeff: 1.0
        use_huber: True
        huber_threshold: 1.0
        l2_reg: 0.000001
        learning_starts: 100000
        sample_batch_size: 50
        train_batch_size: 2048
        min_iter_time_s: 30

        # === Parallelism ===
        num_gpus: 2
        num_workers: 256
